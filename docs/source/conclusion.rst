Conclusion
==========

In this work :cite:`mache2025domain`, we have introduced a domain decomposition of the modified Born series (MBS) approach :cite:`osnabrugge2016convergent, vettenburg2023universal` applied to the Helmholtz equation. With the new framework, we simulated a complex 3D structure of a remarkable :math:`3.27\cdot 10^7` wavelengths in size in just :math:`45` minutes by solving over two GPUs. This result is a factor of :math:`1.95` increase over the largest possible simulation on a single GPU without domain decomposition. Furthermore, we showed that excellent scaling is maintained up to at least four GPUs.

Our decomposition framework hinges on the ability to split the linear system as :math:`A=L+V`. Instead of the traditional splitting, where :math:`V` is a scattering potential that acts locally on each voxel, we introduced a :math:`V` that includes the communication between subdomains and corrections for wraparound artefacts. As a result, the operator :math:`(L+I)^{-1}` in the MBS iteration can be evaluated locally on each subdomain using a fast convolution. Therefore, this operator, which is the most computationally intensive step of the iteration, can be evaluated in parallel on multiple GPUs. 

With the current 2-GPU simulation, we were able to solve a problem of :math:`320\times 320\times 320` wavelengths :math:`20\times` faster than without domain decomposition, as the non-decomposed problem is too large to fit on a single GPU. This marks a significant enhancement to the MBS in terms of performance and the scalability of simulations. However, there are two trade-offs associated with this gain in speed and simulation size due to the overheads associated with our domain splitting method. First, there is a significant overhead in terms of an increased number of iterations to solve the problem with multiple domains than with a single domain. Second, there is also the communication and synchronisation overhead, but there is only a slight overhead associated with adding more subdomains along an axis after the first splitting. This favourable scaling paves the way for distributing simulations over more GPUs or compute nodes in a cluster.

In this work, we have already introduced strategies to reduce the overhead of the domain decomposition through truncating the blocks for subdomain communication and wrapping artefacts correction to only a few points close to the edge of the subdomain, and only activating certain subdomains in the iteration. We anticipate that further developments and optimisation of the code may help reduce the overhead of the lock-step execution.

Finally, due to the generality of our approach, we expect it to be readily extended to include Maxwell's equations :cite:`kruger2017solution` and birefringent media :cite:`vettenburg2019calculating`. Given the rapid developments of GPU hardware and compute clusters, we anticipate that optical simulations at a cubic-millimetre scale can soon be performed in a matter of minutes.

Code availability
-----------------
The code for WaveSim is available on GitHub :cite:`wavesim_py`, it is licensed under the MIT license. When using WaveSim in your work, please cite :cite:`mache2025domain, osnabrugge2016convergent`. Examples and documentation for this project are available at `Read the Docs <https://wavesim.readthedocs.io/en/latest/>`_ :cite:`wavesim_documentation`.

%endmatter%